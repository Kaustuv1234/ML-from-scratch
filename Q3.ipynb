{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c901c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff1185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self,ann_path,im_path):\n",
    "        \n",
    "        self.ann_path = ann_path\n",
    "        self.im_path = im_path\n",
    "    \n",
    "    def get_file_list(self,root, file_type):\n",
    "        return [os.path.join(directory_path, f) for directory_path, directory_name, \n",
    "            files in os.walk(root) for f in files if f.endswith(file_type)]\n",
    "\n",
    "    def get_train_df(self,ann_path, img_path):\n",
    "    \n",
    "        ann_path_list = self.get_file_list(self.ann_path, '.xml')\n",
    "        ann = np.zeros((len(ann_path_list),4))\n",
    "        for i in range(len(ann_path_list)):\n",
    "            a_path = ann_path_list[i]\n",
    "            root = ET.parse(a_path).getroot()\n",
    "            ann[i][0] = int(root.find(\"./object/bndbox/xmin\").text)\n",
    "            ann[i][1] = int(root.find(\"./object/bndbox/ymin\").text)\n",
    "            ann[i][2] = int(root.find(\"./object/bndbox/xmax\").text)\n",
    "            ann[i][3] = int(root.find(\"./object/bndbox/ymax\").text)\n",
    "        return ann\n",
    "\n",
    "    def get_image_data(self):\n",
    "    \n",
    "        image_list = get_file_list(self.im_path,'png')\n",
    "        image_data = [ cv2.imread(image_path) for image_path in image_list]\n",
    "\n",
    "        return image_data\n",
    "    \n",
    "    def resize_image_bounding_box(self):\n",
    "        \n",
    "        image_data = self.get_image_data()\n",
    "        targetSize = (100,100)\n",
    "        image_list = get_file_list(self.im_path,'png')\n",
    "        resized_image_list = []\n",
    "        for i in range(len(image_data)):\n",
    "            x_scale = 100/image_data[i].shape[0]\n",
    "            y_scale = 100/image_data[i].shape[1]\n",
    "            train_box[i][0] = int(np.round(train_box[i][0]*x_scale))\n",
    "            train_box[i][1] = int(np.round(train_box[i][1]*y_scale))\n",
    "            train_box[i][2] = int(np.round(train_box[i][2]*x_scale))\n",
    "            train_box[i][3] = int(np.round(train_box[i][3]*y_scale))\n",
    "            image = cv2.imread(image_list[i])\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) ## grayscale image\n",
    "            norm_image = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) ## normalize\n",
    "            image = cv2.resize(norm_image,targetSize)  ## resize to 100*100\n",
    "            #print(image.shape)\n",
    "            image = np.array(image)\n",
    "            resized_image_list.append(image.ravel())\n",
    "\n",
    "        return train_box,np.array(resized_image_list)  \n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        train_box,resized_image_list = self.resize_image_bounding_box()\n",
    "        return train_test_split(resized_image_list, train_box, test_size=0.3, random_state=34)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "805f3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LinearRegression():\n",
    "    \n",
    "    def __init__(self, learning_rate,alpha,regularization,L1_ratio):\n",
    "            \n",
    "        self.param = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_rate = alpha\n",
    "        self.regularization = regularization\n",
    "        self.L1_ratio = L1_ratio\n",
    "        self.n_iterations = 2000\n",
    "    def initialize_parameters(self, input_dim,output_dim):\n",
    "        \n",
    "        self.param = np.ones((input_dim,output_dim))\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        \n",
    "        l1_ratio = self.L1_ratio\n",
    "        x = np.insert(x, 0, 1, axis=1)\n",
    "        self.training_errors = []\n",
    "        self.MAE = []\n",
    "        self.initialize_parameters(input_dim = x.shape[1],output_dim = y.shape[1])\n",
    "        print(self.param.shape)\n",
    "        # Do gradient descent for n_iterations\n",
    "        for i in tqdm(range(self.n_iterations)):\n",
    "            \n",
    "            self.learning_rate = self.learning_rate/(10**int((i/1000))) ## learning rate decay\n",
    "            y_pred = np.dot(x,self.param)\n",
    "            if self.regularization == None:\n",
    "                mse = np.mean(0.5 * (y - y_pred)**2)\n",
    "                grad_param = np.dot(x.T,(y_pred - y))\n",
    "            elif self.regularization == 'L1':\n",
    "                mse = np.mean(0.5 * (y - y_pred)**2) + self.reg_rate*np.linalg.norm(self.param, ord= 1)\n",
    "                grad_param = np.dot(x.T,(y_pred - y)) + self.reg_rate*np.sign(self.param)\n",
    "            elif self.regularization == 'L2':\n",
    "                mse = np.mean(0.5 * (y - y_pred)**2) + 0.5*self.reg_rate*np.linalg.norm(self.param)**2\n",
    "                grad_param = np.dot(x.T,(y_pred - y)) + self.reg_rate*self.param\n",
    "            elif self.regularization == 'Elastic':\n",
    "                mse = np.mean(0.5 * (y - y_pred)**2) + self.reg_rate*(l1_ratio*np.linalg.norm(self.param, ord= 1)+(1-l1_ratio)*0.5*np.linalg.norm(self.param)**2)\n",
    "                grad_param = np.dot(x.T,(y_pred - y)) + self.reg_rate*((1-l1_ratio)*self.param + l1_ratio*np.sign(self.param))\n",
    "            \n",
    "            self.training_errors.append(mse)\n",
    "            self.MAE.append(np.mean(np.abs(y-y_pred)))\n",
    "            self.param = self.param - self.learning_rate * grad_param\n",
    "        \n",
    "            \n",
    "    def predict(self,x_test):\n",
    "        x_test = np.insert(x_test, 0, 1, axis=1)\n",
    "        pred = np.dot(x_test,self.param)\n",
    "        return pred\n",
    "    \n",
    "    def getMetrics(self,y_pred,actual):\n",
    "        \n",
    "        MoU = 0\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            if y_pred[i][2] < actual[i][0] or y_pred[i][0] > actual[i][2] or y_pred[i][3] < actual[i][1] or y_pred[i][1] > actual[i][3]:\n",
    "                continue\n",
    "            x_min = max(y_pred[i][0],actual[i][0])\n",
    "            x_max = min(y_pred[i][2],actual[i][2])\n",
    "            y_min = max(y_pred[i][1],actual[i][1])\n",
    "            y_max = min(y_pred[i][3],actual[i][3])\n",
    "            intersection = (x_max - x_min)*(y_max-y_min)\n",
    "            union = (y_pred[i][2]-y_pred[i][0])*(y_pred[i][3]-y_pred[i][1]) + (actual[i][2]-actual[i][0])*(actual[i][3]-actual[i][1]) - intersection;\n",
    "            MoU += intersection/union\n",
    "            \n",
    "        MSE = np.mean(0.5 * (actual - y_pred)**2)\n",
    "        MAE = np.mean(np.abs(actual - y_pred))\n",
    "        \n",
    "        return MSE,MoU/y_pred.shape[0],MAE\n",
    "    \n",
    "    def plot_loss(self):\n",
    "    \n",
    "        iters = np.arange(0,self.n_iterations,1)\n",
    "        fig = plt.figure(figsize=(4,4))\n",
    "        plt.plot(iters[50:],self.training_errors[50:],label = 'MSE')\n",
    "        plt.plot(iters[50:],self.MAE[50:],label = 'MAE')\n",
    "        plt.xlabel('No. of iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        if self.regularization == None:\n",
    "            plt.title('Simple Linear Regression')\n",
    "            plt.savefig('Q3_LinearRegression.png')\n",
    "        else:\n",
    "            plt.title(f'{self.regularization}Regression')\n",
    "            plt.savefig(f'Q3{self.regularization}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb7445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:26<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for Simple Linear are (20.378668677854282, 0.0, 4.745390287690216)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:10<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for L1 are (20.368501106409237, 0.0, 4.744235568115445)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:13<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for L2 are (20.378260490081217, 0.0, 4.745342775033581)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:17<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for Elastic are (20.37728439618444, 0.0, 4.745232049585597)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:26<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for Elastic with L1 ratio = 0.1 are (20.37728439618444, 0.0, 4.745232049585597)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:22<00:00, 14.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for Elastic with L1 ratio = 0.30000000000000004 are (20.375332310070746, 0.0, 4.745010601633409)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:21<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for Elastic with L1 ratio = 0.5 are (20.373380376344908, 0.0, 4.7447891595948075)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:18<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for Elastic with L1 ratio = 0.7 are (20.371428563375225, 0.0, 4.744567719566063)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:21<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for Elastic with L1 ratio = 0.8999999999999999 are (20.36947690631594, 0.0, 4.744346285940574)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [20:11<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for L1 with lambda = 0.001 are (20.378566983748193, 0.0, 4.745378739940554)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:21<00:00, 14.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics for L1 with lambda = 0.01 are (20.377651753231454, 0.0, 4.745274810651416)\n",
      "(10001, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████▉                                 | 1151/2000 [01:22<01:00, 13.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(reg_rate\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     72\u001b[0m     clf \u001b[38;5;241m=\u001b[39m LinearRegression(\u001b[38;5;241m1e-7\u001b[39m,reg_rate,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     training_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(clf\u001b[38;5;241m.\u001b[39mtraining_errors)\n\u001b[0;32m     75\u001b[0m     training_MAE \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(clf\u001b[38;5;241m.\u001b[39mMAE)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregularization \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL1\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     34\u001b[0m     mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (y \u001b[38;5;241m-\u001b[39m y_pred)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_rate\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam, \u001b[38;5;28mord\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     grad_param \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_rate\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msign(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregularization \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     37\u001b[0m     mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (y \u001b[38;5;241m-\u001b[39m y_pred)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_rate\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    '''\n",
    "    ann_path = '../Assignment 1/data/Road Sign Detection/annotations/'\n",
    "    image_path = '../Assignment 1/data/Road Sign Detection/images/'\n",
    "    dataset = DataLoader(ann_path,image_path)\n",
    "    X_train, X_test, y_train, y_test = dataset.train_test_split()\n",
    "    data = [X_train, X_test, y_train, y_test]\n",
    "    \n",
    "    with open('Q3.pkl', 'wb') as outfile:\n",
    "        pickle.dump(data, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "    '''\n",
    "        \n",
    "    with open('Q3.pkl', 'rb') as infile:\n",
    "        result = pickle.load(infile)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = result\n",
    "    \n",
    "    \n",
    "    clf = LinearRegression(1e-7,None,None,None)\n",
    "    clf.fit(X_train,y_train)\n",
    "    training_errors = np.array(clf.training_errors)\n",
    "    training_MAE = np.array(clf.MAE)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    MSE,mIoU,MAE = clf.getMetrics(y_pred,y_test) \n",
    "    print(f'metrics for Simple Linear are {MSE,mIoU,MAE}')\n",
    "    clf.plot_loss()\n",
    "    \n",
    "    \n",
    "    clf = LinearRegression(1e-7,0.1,'L1',None)\n",
    "    clf.fit(X_train,y_train)\n",
    "    training_errors = np.array(clf.training_errors)\n",
    "    training_MAE = np.array(clf.MAE)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    MSE,mIoU,MAE = clf.getMetrics(y_pred,y_test) \n",
    "    print(f'metrics for L1 are {MSE,mIoU,MAE}')\n",
    "    clf.plot_loss()\n",
    "    \n",
    "    clf = LinearRegression(1e-7,0.1,'L2',None)\n",
    "    clf.fit(X_train,y_train)\n",
    "    training_errors = np.array(clf.training_errors)\n",
    "    training_MAE = np.array(clf.MAE)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    MSE,mIoU,MAE = clf.getMetrics(y_pred,y_test) \n",
    "    print(f'metrics for L2 are {MSE,mIoU,MAE}')\n",
    "    clf.plot_loss()\n",
    "    \n",
    "    clf = LinearRegression(1e-7,0.1,'Elastic',0.1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    training_errors = np.array(clf.training_errors)\n",
    "    training_MAE = np.array(clf.MAE)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    MSE,mIoU,MAE = clf.getMetrics(y_pred,y_test) \n",
    "    print(f'metrics for Elastic are {MSE,mIoU,MAE}')\n",
    "    clf.plot_loss()\n",
    "    \n",
    "    ## L1 ratio tuning for Elastic Net\n",
    "    L1_ratio = 0.1\n",
    "    while(L1_ratio<1):\n",
    "        clf = LinearRegression(1e-7,0.1,'Elastic',L1_ratio)\n",
    "        clf.fit(X_train,y_train)\n",
    "        training_errors = np.array(clf.training_errors)\n",
    "        training_MAE = np.array(clf.MAE)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        MSE,mIoU,MAE = clf.getMetrics(y_pred,y_test) \n",
    "        print(f'metrics for Elastic with L1 ratio = {L1_ratio} are {MSE,mIoU,MAE}')\n",
    "        L1_ratio += 0.2\n",
    "    \n",
    "    ## L1 regression Regularization rate tuning\n",
    "    reg_rate = 0.001\n",
    "    while(reg_rate<1):\n",
    "        clf = LinearRegression(1e-7,reg_rate,'L1',None)\n",
    "        clf.fit(X_train,y_train)\n",
    "        training_errors = np.array(clf.training_errors)\n",
    "        training_MAE = np.array(clf.MAE)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        MSE,mIoU,MAE = clf.getMetrics(y_pred,y_test) \n",
    "        print(f'metrics for L1 with lambda = {reg_rate} are {MSE,mIoU,MAE}')\n",
    "        reg_rate *= 10\n",
    "    \n",
    "    \n",
    "    ##  L1 regression Regularization rate tuning\n",
    "    \n",
    "    reg_rate = 0.001\n",
    "    while(reg_rate<1):\n",
    "        clf = LinearRegression(1e-7,reg_rate,'L2',None)\n",
    "        clf.fit(X_train,y_train)\n",
    "        training_errors = np.array(clf.training_errors)\n",
    "        training_MAE = np.array(clf.MAE)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        MSE,mIoU,MAE = clf.getMetrics(y_pred,y_test) \n",
    "        print(f'metrics for L2 with lambda = {reg_rate} are {MSE,mIoU,MAE}')\n",
    "        reg_rate *= 10\n",
    "    \n",
    "    ## Elastic Net hyper parameter tuning\n",
    "    \n",
    "    reg_rate = 0.001\n",
    "    while(reg_rate<1):\n",
    "        clf = LinearRegression(1e-7,reg_rate,'Elastic',0.1)\n",
    "        clf.fit(X_train,y_train)\n",
    "        training_errors = np.array(clf.training_errors)\n",
    "        training_MAE = np.array(clf.MAE)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        MSE,mIoU,MAE = clf.getMetrics(y_pred,y_test) \n",
    "        print(f'metrics for Elastic with lambda = {reg_rate} are {MSE,mIoU,MAE}')\n",
    "        reg_rate *= 10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path = \n",
    "image_path = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555945d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_box = get_train_df(annotation_path,image_path)\n",
    "print(train_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c1fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93384c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = get_image_data(image_path)\n",
    "print(image_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4cb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_box,resized_image_list = resize_image_bounding_box(image_data)\n",
    "print(resized_image_list.shape)\n",
    "print(train_box.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(resized_image_list, train_box, test_size=0.3, random_state=34)\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577f243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e91f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417c6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbda66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6786e74f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56beb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd9d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_name",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
