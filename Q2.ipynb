{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bd9412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab88e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiclass_Gaussian_Naive_Bayes():\n",
    "    \n",
    "    def __init__(self,n_features,n_classes):\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.mean = np.zeros((n_classes,n_features))\n",
    "        self.variance = np.zeros((n_classes,n_features))\n",
    "    \n",
    "    def fit(self,X,counts):\n",
    "        for i in range(np.size(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            self.mean[i] = np.mean(curr_X,axis = 0)\n",
    "            diff = curr_X - self.mean[i]\n",
    "            self.variance[i] = np.mean(np.square(diff),axis = 0)\n",
    "\n",
    "    def gaussian(self,X, mu, var):\n",
    "        return 1 / ((2 * np.pi) ** (1 / 2) * var ** 0.5) * np.exp(-0.5 * (X-mu)**2/var)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for j in range(x_test.shape[0]):\n",
    "            best_class = 0\n",
    "            best_likelihood = -math.inf\n",
    "            for i in range(self.n_classes):\n",
    "                likelihood = self.gaussian(x_test[j],self.mean[i],self.variance[i])\n",
    "                log_likelihood = np.sum(np.log(likelihood))\n",
    "                if best_likelihood < log_likelihood:\n",
    "                    best_likelihood = log_likelihood\n",
    "                    best_class = i\n",
    "            pred[j] = best_class\n",
    "        return pred\n",
    "                \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc32ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class Multiclass_Logistic_Regression():\n",
    "    \n",
    "    def __init__(self, learning_rate,n_classes):\n",
    "        self.param = None\n",
    "        self.lr = learning_rate\n",
    "        self.training_errors = []\n",
    "        self.eps = 1e-7\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        self.param = np.ones((self.n_classes,n_features))\n",
    "    \n",
    "    def one_hot(self,y):\n",
    "\n",
    "        return np.eye(self.n_classes)[y.reshape(-1)]\n",
    "    '''\n",
    "    def softmax(self,probs):\n",
    "        probs = probs - (np.mean(probs,axis=1)).reshape(-1,1)   ## normalization of probs\n",
    "        return np.exp(probs)/(np.sum(np.exp(probs),axis = 1) + self.eps).reshape(-1,1)\n",
    "    '''\n",
    "\n",
    "    def fit(self, X, y, n_iterations=1000):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        y = self.one_hot(y)\n",
    "        self.initialize_parameters(X)\n",
    "        loss_per_iter = []\n",
    "        for i in range(n_iterations):\n",
    "            y_pred = softmax(np.dot(X,self.param.T),axis = 1)\n",
    "            loss = -1*np.mean(y*np.log(y_pred + self.eps))\n",
    "            loss_per_iter.append(loss)\n",
    "            grad = np.dot((y_pred-y).T,X)\n",
    "            self.param = self.param - self.lr*grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        y_pred = np.argmax(softmax(np.dot(X,self.param.T),axis=1),axis = 1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e888fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor():\n",
    "    \n",
    "    def __init__(self,x_train,y_train,K):\n",
    "        self.k = K\n",
    "        self.X = x_train\n",
    "        self.y = y_train\n",
    "    \n",
    "    def get_Euclidean_distance(self,x):\n",
    "        \n",
    "        return np.sqrt(np.sum((self.X - x)**2,axis=1))\n",
    "        \n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in range(x_test.shape[0]):\n",
    "            dist = self.get_Euclidean_distance(x_test[i])\n",
    "            nearest_neighbors = dist.argsort()[0:self.k]\n",
    "            unique,counts = np.unique(self.y[nearest_neighbors], return_counts = True)\n",
    "            pred[i] = unique[np.argmax(counts)]\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e4c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_LDA():\n",
    "\n",
    "    def __init__(self,n_features,n_classes):\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.S_w = np.zeros((n_features,n_features))\n",
    "        self.S_b = np.zeros((n_features,n_features))\n",
    "        self.mu = np.zeros(n_features)\n",
    "        self.mu_class = np.zeros((self.n_classes,self.n_features))\n",
    "\n",
    "    def fit(self,X,counts):\n",
    "        \n",
    "        self.mu = np.mean(X,axis = 0)\n",
    "        for i in range(self.n_classes):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            self.mu_class[i] = np.mean(curr_X,axis = 0)\n",
    "            self.S_w = self.S_w + np.dot((curr_X - self.mu_class[i]).T,curr_X - self.mu_class[i])\n",
    "            self.S_b = self.S_b + counts[i]*np.dot((self.mu - self.mu_class[i]).T,self.mu - self.mu_class[i])\n",
    "     \n",
    "    def predict(self,x_test,n_components):\n",
    "        \n",
    "        V = np.dot(np.linalg.inv(self.S_w),self.S_b)\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(V)\n",
    "        eigenvectors = eigenvectors.T\n",
    "        eigenList = [(eigenvalues[i],eigenvectors[i,:]) for i in range(len(eigenvalues))]\n",
    "        eigenList = sorted(eigenList,key = lambda x:x[0] ,reverse= True)\n",
    "        for i in range(n_components):\n",
    "            eigenvectors[i,:] = eigenList[i][1]\n",
    "        # Project the data onto eigenvectors\n",
    "        eigenvectors = eigenvectors[0:n_components,:]\n",
    "        projected_X = np.dot(x_test,eigenvectors.T)\n",
    "        print(eigenvectors.shape,projected_X.shape)\n",
    "        projected_mu = np.dot(self.mu_class,eigenvectors.T)\n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in range(x_test.shape[0]):\n",
    "            best_class = 0\n",
    "            best_dist = math.inf\n",
    "            for j in range(self.n_classes):\n",
    "                if np.linalg.norm(projected_X[i] - projected_mu[j]) < best_dist:\n",
    "                    best_dist = np.linalg.norm(projected_X[i] - projected_mu[j])\n",
    "                    best_class = j\n",
    "            pred[i] = best_class\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69d01f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMLE():\n",
    "    \n",
    "    def __init__(self,n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.eps = 1e-7\n",
    "    \n",
    "    def fit(self,X,counts):\n",
    "        self.mu = np.zeros((self.n_classes,X.shape[1]))\n",
    "        self.cov = np.zeros((self.n_classes,X.shape[1],X.shape[1]))\n",
    "        \n",
    "        for i in range(len(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            self.mu[i] = np.mean(curr_X,axis = 0)\n",
    "            self.cov[i] = np.dot((curr_X-self.mu[i]).T,curr_X-self.mu[i])\n",
    "            \n",
    "    def log_likelihood(self,X, mu, cov):\n",
    "        \n",
    "        sign,log_det = np.linalg.slogdet(cov)\n",
    "\n",
    "        return 0.5*log_det -0.5*np.dot(np.dot((X-mu).T,np.linalg.inv(cov)),X-mu)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in range(x_test.shape[0]):\n",
    "            best_class = 0\n",
    "            best_likelihood = -math.inf\n",
    "            for j in range(self.n_classes):\n",
    "                log_likelihood = self.log_likelihood(x_test[i],self.mu[j],self.cov[j])\n",
    "                if best_likelihood < log_likelihood:\n",
    "                    best_likelihood = log_likelihood\n",
    "                    best_class = j\n",
    "            pred[i] = best_class\n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "461fd206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self,data_path):\n",
    "        self.data = np.load(data_path)\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        x_train = self.data['train_images'].reshape(self.data['train_images'].shape[0],-1)\n",
    "        x_test = self.data['test_images'].reshape(self.data['test_images'].shape[0],-1)\n",
    "        x_val = self.data['val_images'].reshape(self.data['val_images'].shape[0],-1)\n",
    "        y_train = self.data['train_labels']\n",
    "        y_test = self.data['test_labels']\n",
    "        y_val = self.data['val_labels']\n",
    "    \n",
    "        return x_train,y_train,x_test,y_test,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c41c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    dataset = DataLoader('../Assignment 1/data/bloodmnist.npz')\n",
    "    x_train,y_train,x_test,y_test,x_val,y_val = dataset.train_test_split()\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    X = np.zeros(x_train.shape)\n",
    "    y = np.zeros(y_train.shape[0],dtype = int)\n",
    "    idx = 0\n",
    "    for label in range(8):\n",
    "        labels = np.where(y_train==label)[0]\n",
    "        for i in range(np.size(labels)):\n",
    "            X[idx] = x_train[labels[i]]\n",
    "            y[idx] = label\n",
    "            idx += 1\n",
    "    \n",
    "    '''\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X,y)\n",
    "    print(f'accuracy of Sklearn Gaussian Naive Bayes = {clf.score(x_test,y_test)}')\n",
    "    \n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(X,y)\n",
    "    print(f'accuracy of Sklearn LDA = {clf.score(x_test,y_test)}')\n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X,y)\n",
    "    print(f'accuracy of Sklearn Logisitic regression = {clf.score(x_test,y_test)}')\n",
    "    '''\n",
    "    \n",
    "    clf = GaussianMLE(n_classes=8)\n",
    "    clf.fit(X,counts)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of my Gaussian MLE = {accuracy}',pred)\n",
    "    \n",
    "    clf =  Multiclass_Logistic_Regression(learning_rate = 0.001, n_classes=8)\n",
    "    clf.fit(X,y)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of my MultiClass Logistic Regression = {accuracy}',pred) \n",
    "    \n",
    "    \n",
    "    clf = Multiclass_Gaussian_Naive_Bayes(x_train.shape[1],8)\n",
    "    clf.fit(X,counts)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of my MultiClass Gaussian Naive Bayes = {accuracy}',pred) \n",
    "    \n",
    "    \n",
    "    clf = Multi_LDA(n_classes=8,n_features = x_train.shape[1])\n",
    "    clf.fit(X,counts)\n",
    "    pred = clf.predict(x_test,n_components = 7)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of my LDA with n_components = 7  is {accuracy}',pred)\n",
    "    \n",
    "    clf = KNearestNeighbor(X,y,1)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of K nearest neighbor = {accuracy}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e2fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b92b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_name",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
