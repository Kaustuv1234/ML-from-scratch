{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "676f9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33b30e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    \n",
    "    def __init__(self,data_path):\n",
    "        self.data = np.load(data_path)\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \n",
    "        x_train = self.data['train_images'].reshape(self.data['train_images'].shape[0],-1)\n",
    "        x_test = self.data['test_images'].reshape(self.data['test_images'].shape[0],-1)\n",
    "        x_val = self.data['val_images'].reshape(self.data['val_images'].shape[0],-1)\n",
    "        y_train = self.data['train_labels']\n",
    "        y_test = self.data['test_labels']\n",
    "        y_val = self.data['val_labels']\n",
    "        \n",
    "        return x_train,y_train,x_test,y_test,x_val,y_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf3b23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiclass_Gaussian_Naive_Bayes():\n",
    "    \n",
    "    def __init__(self,n_features,n_classes):\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.mean = np.zeros((n_classes,n_features))\n",
    "        self.variance = np.zeros((n_classes,n_features))\n",
    "        self.eps = 1e-7\n",
    "    def fit(self,X,counts):\n",
    "    \n",
    "        for i in range(np.size(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            self.mean[i] = np.mean(curr_X,axis = 0)\n",
    "            diff = curr_X - self.mean[i]\n",
    "            self.variance[i] = np.mean(np.square(diff),axis = 0)\n",
    "\n",
    "    def gaussian(self,X, mu, var):\n",
    "        \n",
    "        return 1 / ((2 * np.pi) ** (1 / 2) * var ** 0.5) * np.exp(-0.5 * (X-mu)**2/var)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        \n",
    "        for j in range(x_test.shape[0]):\n",
    "            best_class = 0\n",
    "            best_likelihood = -math.inf\n",
    "            for i in range(self.n_classes):\n",
    "                likelihood = self.gaussian(x_test[j],self.mean[i],self.variance[i])\n",
    "                log_likelihood = np.sum(np.log(likelihood+self.eps))\n",
    "                if best_likelihood < log_likelihood:\n",
    "                    best_likelihood = log_likelihood\n",
    "                    best_class = i\n",
    "            pred[j] = best_class\n",
    "        return pred\n",
    "                \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ab1792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    \n",
    "    def __init__(self,n_features,n_classes):\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.S_w = np.zeros((n_features,n_features))\n",
    "        self.S_b = np.zeros((n_features,n_features))\n",
    "        self.mu = np.zeros(n_features)\n",
    "        self.mu_class = np.zeros((self.n_classes,self.n_features))\n",
    "\n",
    "    def fit(self,X,counts):\n",
    "        self.mu = np.mean(X,axis = 0)\n",
    "        self.mu_class[0] = np.mean(X[0:counts[0],:],axis = 0)\n",
    "        self.mu_class[1] = np.mean(X[counts[0]:,:],axis = 0)\n",
    "        self.S_w = np.dot((X[0:counts[0],:]-self.mu_class[0]).T,X[0:counts[0],:]-self.mu_class[0]) + np.dot((X[counts[0]:,:]-self.mu_class[1]).T,X[counts[0]:,:]-self.mu_class[1])\n",
    "        self.S_b = np.dot((self.mu_class[0] - self.mu_class[1]).T,self.mu_class[0] - self.mu_class[1])\n",
    "            \n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        v = np.dot(np.linalg.inv(self.S_w),self.mu_class[0] - self.mu_class[1])\n",
    "        pred = [1 if abs(np.dot(x_test[i],v)-np.dot(self.mu_class[0],v))> abs(np.dot(x_test[i],v)-np.dot(self.mu_class[1],v)) else 0 for i in range(x_test.shape[0])]\n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "731d657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        self.param = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.training_errors = []\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        self.param = np.ones((n_features,1))\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def fit(self, X, y, n_iterations=4000):\n",
    "        X = np.insert(X,0,1,axis = 1)\n",
    "        y = y.reshape(-1,1)\n",
    "        self.initialize_parameters(X)\n",
    "        for i in range(n_iterations):\n",
    "            \n",
    "            y_pred = self.sigmoid(np.dot(X,self.param))\n",
    "            self.training_errors.append(np.dot(y.T,np.log(y_pred + self.eps)).item())\n",
    "            self.param -= self.learning_rate * -np.dot(X.T,y - y_pred)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis = 1)\n",
    "        y_pred = np.round(self.sigmoid(np.dot(X,self.param))).astype(int)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8e3a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "class Multiclass_Logistic_Regression():\n",
    "    \n",
    "    def __init__(self, learning_rate,n_classes):\n",
    "        self.param = None\n",
    "        self.lr = learning_rate\n",
    "        self.training_errors = []\n",
    "        self.eps = 1e-7\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        self.param = np.ones((self.n_classes,n_features))\n",
    "    \n",
    "    def one_hot(self,y):\n",
    "\n",
    "        return np.eye(self.n_classes)[y.reshape(-1)]\n",
    "    '''\n",
    "    def softmax(self,probs):\n",
    "        probs = probs - (np.mean(probs,axis=1)).reshape(-1,1)   ## normalization of probs\n",
    "        return np.exp(probs)/(np.sum(np.exp(probs),axis = 1) + self.eps).reshape(-1,1)\n",
    "    '''\n",
    "\n",
    "    def fit(self, X, y, n_iterations=1000):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        y = self.one_hot(y)\n",
    "        self.initialize_parameters(X)\n",
    "        loss_per_iter = []\n",
    "        for i in range(n_iterations):\n",
    "            y_pred = softmax(np.dot(X,self.param.T),axis = 1)\n",
    "            loss = -1*np.mean(y*np.log(y_pred + self.eps))\n",
    "            loss_per_iter.append(loss)\n",
    "            grad = np.dot((y_pred-y).T,X)\n",
    "            self.param = self.param - self.lr*grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.insert(X,0,1,axis=1)\n",
    "        y_pred = np.argmax(softmax(np.dot(X,self.param.T),axis=1),axis = 1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8353a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMLE():\n",
    "    \n",
    "    def __init__(self,n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.eps = 1e-7\n",
    "    \n",
    "    def fit(self,X,counts):\n",
    "        \n",
    "        self.mu = np.zeros((self.n_classes,X.shape[1]))\n",
    "        self.cov = np.zeros((self.n_classes,X.shape[1],X.shape[1]))\n",
    "        \n",
    "        for i in range(len(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            self.mu[i] = np.mean(curr_X,axis = 0)\n",
    "            self.cov[i] = np.dot((curr_X-self.mu[i]).T,curr_X-self.mu[i])\n",
    "            \n",
    "    def log_likelihood(self,X, mu, cov):\n",
    "        \n",
    "        sign,log_det = np.linalg.slogdet(cov)\n",
    "        det = sign*np.exp(log_det)\n",
    "        return 0.5*log_det -0.5*np.dot(np.dot((X-mu).T,np.linalg.inv(cov)),X-mu)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in range(x_test.shape[0]):\n",
    "            best_class = 0\n",
    "            best_likelihood = -math.inf\n",
    "            for j in range(self.n_classes):\n",
    "                log_likelihood = self.log_likelihood(x_test[i],self.mu[j],self.cov[j])\n",
    "                if best_likelihood < log_likelihood:\n",
    "                    best_likelihood = log_likelihood\n",
    "                    best_class = j\n",
    "            pred[i] = best_class\n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09069b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor():\n",
    "    \n",
    "    def __init__(self,x_train,y_train,K):\n",
    "        self.k = K\n",
    "        self.X = x_train\n",
    "        self.y = y_train\n",
    "    \n",
    "    def get_Euclidean_distance(self,x):\n",
    "        \n",
    "        return np.sqrt(np.sum((self.X - x)**2,axis=1))\n",
    "        \n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in range(x_test.shape[0]):\n",
    "            dist = self.get_Euclidean_distance(x_test[i])\n",
    "            nearest_neighbors = dist.argsort()[0:self.k]\n",
    "            unique,counts = np.unique(self.y[nearest_neighbors], return_counts = True)\n",
    "            pred[i] = unique[np.argmax(counts)]\n",
    "            \n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e213e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM():\n",
    "    \n",
    "    def __init__(self,n_clusters):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.clusters = []\n",
    "        self.eps = 1e-7\n",
    "        \n",
    "    def gaussian(self,X, mu, cov):\n",
    "        n = X.shape[1]\n",
    "        scaled_probs = np.zeros((X.shape[0],1))\n",
    "        sign,det = np.linalg.slogdet(cov)\n",
    "        for i in range(X.shape[0]):\n",
    "            diff = (X[i] - mu).reshape(-1,1)\n",
    "            scaled_probs[i] = np.exp(0.5*(det - np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff)))\n",
    "        return scaled_probs\n",
    "        \n",
    "    def initialize_clusters(self,n_features):\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        for i in range(self.n_clusters):\n",
    "            self.clusters.append({\n",
    "                'pi_k': 1.0 / self.n_clusters,\n",
    "                'mu_k': np.ones(self.n_features),\n",
    "                'cov_k': np.identity(self.n_features, dtype=np.float64)\n",
    "            })\n",
    "    \n",
    "    def expectation(self,X):\n",
    "        totals = np.zeros((X.shape[0], 1), dtype=np.float64)\n",
    "        for cluster in self.clusters:\n",
    "        \n",
    "            probs = (cluster['pi_k'] * self.gaussian(X, cluster['mu_k'], cluster['cov_k'])).astype(np.float64)\n",
    "\n",
    "            for i in range(X.shape[0]):\n",
    "                totals[i] += probs[i]\n",
    "\n",
    "            cluster['probs'] = probs\n",
    "            cluster['totals'] = totals\n",
    "\n",
    "\n",
    "        for cluster in self.clusters:\n",
    "            cluster['probs'] = cluster['probs']/(cluster['totals'] + self.eps)\n",
    "            \n",
    "    def maximization(self,X,n_epochs):\n",
    "        \n",
    "        N = float(X.shape[0])\n",
    "        for cluster in self.clusters:\n",
    "            probs = cluster['probs']\n",
    "            cov_k = np.zeros((X.shape[1], X.shape[1]))\n",
    "            N_k = np.sum(cluster['probs'])\n",
    "\n",
    "            pi_k = N_k / N\n",
    "            mu_k = np.sum(cluster['probs'] * X, axis=0) / (N_k + self.eps)\n",
    "\n",
    "            for j in range(X.shape[0]):\n",
    "                diff = (X[j] - mu_k).reshape(-1, 1)\n",
    "                cov_k += probs[j] * np.dot(diff, diff.T)\n",
    "\n",
    "            cov_k = cov_k/(N_k + self.eps)\n",
    "\n",
    "            cluster['pi_k'] = pi_k\n",
    "            cluster['mu_k'] = mu_k\n",
    "            cluster['cov_k'] = cov_k\n",
    "            \n",
    "        \n",
    "    def get_likelihood(self,X):\n",
    "\n",
    "        sample_likelihoods = np.log(np.array([cluster['totals'] for cluster in self.clusters]) + self.eps)\n",
    "        return np.sum(sample_likelihoods), sample_likelihoods\n",
    "    \n",
    "    def train_gmm(self,X,n_epochs):\n",
    "    \n",
    "        self.initialize_clusters(n_features=X.shape[1])\n",
    "        likelihoods = np.zeros((n_epochs, ))\n",
    "        scores = np.zeros((X.shape[0], self.n_clusters))\n",
    "\n",
    "        for i in range(n_epochs):\n",
    "\n",
    "            self.expectation(X)\n",
    "            self.maximization(X,n_epochs)\n",
    "\n",
    "            likelihood, sample_likelihoods = self.get_likelihood(X)\n",
    "            likelihoods[i] = likelihood\n",
    "\n",
    "            #print('Epoch: ', i + 1, 'Likelihood: ', likelihood)\n",
    "\n",
    "        for i, cluster in enumerate(self.clusters):\n",
    "            scores[:, i] = np.log(cluster['probs']).reshape(-1)\n",
    "\n",
    "        return self.clusters, likelihoods, scores, sample_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33a2b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM_classification():\n",
    "    \n",
    "    def __init__(self,n_classes,n_clusters):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_clusters = n_clusters\n",
    "    \n",
    "    def fit(self,X,counts):\n",
    "        \n",
    "        self.class_conditional_densities = []\n",
    "        for i in range(len(counts)):\n",
    "            if i==0:\n",
    "                st_idx = 0\n",
    "            else:\n",
    "                st_idx = np.sum(counts[0:i])\n",
    "            curr_X = X[st_idx:st_idx+counts[i]]\n",
    "            clf = GMM(self.n_clusters)\n",
    "            \n",
    "            clusters = clf.train_gmm(curr_X,100)[0]\n",
    "            self.class_conditional_densities.append(clusters)\n",
    "            \n",
    "    def log_likelihood(self,X, clusters):\n",
    "        \n",
    "        for cluster in clusters:\n",
    "            \n",
    "        #det = sign*np.exp(log_det)\n",
    "        return 0.5*log_det -0.5*np.dot(np.dot((X-mu).T,np.linalg.inv(cov)),X-mu)\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        \n",
    "        pred = np.zeros(x_test.shape[0])\n",
    "        for i in range(x_test.shape[0]):\n",
    "            best_class = 0\n",
    "            best_likelihood = -math.inf\n",
    "            for j in range(self.n_classes):\n",
    "                mu = self.class_conditional_densities[j]['mu_k']\n",
    "                cov = self.class_conditional_densities[j]['cov_k']\n",
    "                log_likelihood = self.log_likelihood(x_test[i],clusters)\n",
    "                if best_likelihood < log_likelihood:\n",
    "                    best_likelihood = log_likelihood\n",
    "                    best_class = j\n",
    "            pred[i] = best_class\n",
    "        return pred\n",
    "    \n",
    "    def accuracy(self,pred,actual):\n",
    "        \n",
    "        n_correct_preds = 0\n",
    "        for i in range(actual.shape[0]):\n",
    "            if pred[i] == actual[i]:\n",
    "                n_correct_preds += 1\n",
    "        accuracy = n_correct_preds/actual.shape[0]\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b90f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    dataset = DataLoader('../Assignment 1/pneumoniamnist.npz')\n",
    "    x_train,y_train,x_test,y_test,x_val,y_val = dataset.train_test_split()\n",
    "    #x_train = dataset.normalize_input(x_train)\n",
    "    unique,counts = np.unique(y_train,return_counts= True)\n",
    "    X = np.zeros(x_train.shape)\n",
    "    y = np.zeros(y_train.shape[0],dtype = int)\n",
    "    idx = 0\n",
    "    for label in range(2):\n",
    "        labels = np.where(y_train==label)[0]\n",
    "        for i in range(np.size(labels)):\n",
    "            X[idx] = x_train[labels[i]]\n",
    "            y[idx] = label\n",
    "            idx += 1\n",
    "    \n",
    "    \n",
    "    clf = GaussianMLE(n_classes=2)\n",
    "    clf.fit(X,counts)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of my Gaussian MLE = {accuracy}') \n",
    "    \n",
    "    clf = KNearestNeighbor(X,y,10)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of K nearest neighbor = {accuracy}')\n",
    "    \n",
    "    \n",
    "    clf = Multiclass_Logistic_Regression(learning_rate= 0.001,n_classes=2)\n",
    "    clf.fit(X,y)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of my Multiclass Logisitic Regression = {accuracy}') \n",
    "    \n",
    "    \n",
    "    clf = LDA(n_classes=2,n_features = x_train.shape[1])\n",
    "    clf.fit(X,counts)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of LDA = {accuracy}')\n",
    "    \n",
    "    clf = Multiclass_Gaussian_Naive_Bayes(x_train.shape[1],2)\n",
    "    clf.fit(X,counts)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of Gaussian_Naive_Bayes = {accuracy}')\n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X,y)\n",
    "    pred = clf.predict(x_test)\n",
    "    accuracy = clf.accuracy(pred,y_test)\n",
    "    print(f'accuracy of Logistic Regression = {accuracy}')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be4c28",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## gaussian Kernel Density estimation from scipy\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "class0_kde = gaussian_kde(x0.T,bw_method='scott').evaluate(x_test.T)\n",
    "class1_kde = gaussian_kde(x1.T,bw_method='scott').evaluate(x_test.T)\n",
    "pred_kde_estimate = np.zeros(x_test.shape[0])\n",
    "\n",
    "n_correct_preds = 0\n",
    "for i in range(y_test.shape[0]):\n",
    "    \n",
    "    if class0_kde[i] > class1_kde[i]:\n",
    "        pred_kde_estimate[i] = 0\n",
    "    else:\n",
    "        pred_kde_estimate[i] = 1\n",
    "    if pred_kde_estimate[i] == y_test[i]:\n",
    "        n_correct_preds += 1\n",
    "        \n",
    "accuracy = n_correct_preds/y_test.shape[0]\n",
    "print(accuracy,pred_kde_estimate,y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b83b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "test_pdf0 = [np.mean(multivariate_normal.pdf(x0,x_test[i],np.identity(x0.shape[1]))) for i in range(x_test.shape[0])]\n",
    "test_pdf1 = [np.mean(multivariate_normal.pdf(x1,x_test[i],np.identity(x1.shape[1]))) for i in range(x_test.shape[0])]\n",
    "\n",
    "pred_Parzen = []\n",
    "correct_preds = 0\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    if test_pdf1[i]>test_pdf0[i]:\n",
    "        pred_Parzen.append(1)\n",
    "        if y_test[i] == 1:\n",
    "            correct_preds += 1\n",
    "    else:\n",
    "        pred_Parzen.append(0)\n",
    "        if y_test[i] == 0:\n",
    "            correct_preds += 0\n",
    "print(correct_preds,pred_Parzen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c31078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16745a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e11be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac949467",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3af91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d456f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e3c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d9048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b96c29",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfadaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_name",
   "language": "python",
   "name": "venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
